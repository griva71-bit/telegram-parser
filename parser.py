import feedparser
import gspread
import json
import os
import re
import requests
import logging
from bs4 import BeautifulSoup
from oauth2client.service_account import ServiceAccountCredentials
from urllib.parse import urlparse

logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s")
log = logging.getLogger(__name__)

SPREADSHEET_ID = os.environ.get("SPREADSHEET_ID", "1VAc6d7sfScnLS-LvN7gYg06IeaR6yLgHAFYv4lcTZ3c")
SHEET_NAME = "news"

RSS_FEEDS = [
    "https://news.google.com/rss/search?q=%D0%B3%D0%BE%D0%BB%D0%BE%D0%B4%D0%B0%D0%BD%D0%B8%D0%B5%20OR%20%D0%B0%D1%83%D1%82%D0%BE%D1%84%D0%B0%D0%B3%D0%B8%D1%8F&hl=ru&gl=RU&ceid=RU:ru",
    "https://medvestnik.ru/rss",
]

ALLOW_KEYWORDS = [
    "–ø—Ä–µ–±–∏–æ—Ç", "–∂–∫—Ç", "–∏–Ω—Ç–µ—Ä–º–∏—Ç–µ–Ω—Ç", "–∏–Ω—Ç–µ—Ä–≤–∞–ª—å–Ω", "–∫–ª–∏–Ω–∏—á–µ—Å–∫",
    "–∫–æ—Ä—Ç–∏–∑–æ–ª", "–∫–µ—Ç–æ–∑", "–≥–æ–ª–æ–¥–∞–Ω", "–∞—É—Ç–æ—Ñ–∞–≥", "–º–∏–∫—Ä–æ–±–∏–æ–º",
    "–º–µ—Ç–∞–±–æ–ª", "–¥–æ–ª–≥–æ–ª–µ—Ç", "–¥–∏–µ—Ç", "–∏–Ω—Å—É–ª–∏–Ω", "fasting", "autophagy"
]

BLOCK_KEYWORDS = [
    "—Ä–µ–º–æ–Ω—Ç", "—É–≤–æ–ª", "–Ω–∞–∑–Ω–∞—á", "–∑–∞–∫—É–ø", "—Ñ–∏–Ω–∞–Ω—Å", "–∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ü",
    "–≥–ª–∞–≤–≤—Ä–∞—á", "—Å–æ–≤–µ—â–∞–Ω", "–∑–∞—Å–µ–¥–∞–Ω", "–∞–∫—Ç—Ä–∏—Å", "—Ä–æ–º–∞–Ω", "—Å–∫–∞–Ω–¥–∞–ª",
    "–∑–≤–µ–∑–¥", "–∑–Ω–∞–º–µ–Ω–∏—Ç", "—Å–≤–µ—Ç—Å–∫"
]

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
}

def is_allowed(text: str) -> bool:
    ltext = (text or "").lower()
    return any(kw in ltext for kw in ALLOW_KEYWORDS) and not any(bw in ltext for bw in BLOCK_KEYWORDS)

def resolve_google_url(url: str) -> str:
    """–†–∞—Å–∫—Ä—ã–≤–∞–µ—Ç —Ä–µ–¥–∏—Ä–µ–∫—Ç Google News –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–µ–∞–ª—å–Ω—ã–π URL —Å—Ç–∞—Ç—å–∏"""
    try:
        if "news.google.com" in url:
            resp = requests.get(url, headers=HEADERS, timeout=10, allow_redirects=True)
            real_url = resp.url
            # –ò–Ω–æ–≥–¥–∞ Google –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
            if "news.google.com" in real_url:
                soup = BeautifulSoup(resp.text, "html.parser")
                # –ò—â–µ–º —Ä–µ–∞–ª—å–Ω—É—é —Å—Å—ã–ª–∫—É –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
                link = soup.find("a", {"data-n-au": True})
                if link:
                    return link["data-n-au"]
                # –ò–ª–∏ –≤ –º–µ—Ç–∞-—Ä–µ–¥–∏—Ä–µ–∫—Ç–µ
                meta = soup.find("meta", {"http-equiv": "refresh"})
                if meta and meta.get("content"):
                    match = re.search(r'url=(.+)', meta["content"], re.I)
                    if match:
                        return match.group(1).strip()
            log.info(f"üîó –†–µ–∞–ª—å–Ω—ã–π URL: {real_url[:80]}")
            return real_url
    except Exception as ex:
        log.error(f"‚ùå –û—à–∏–±–∫–∞ resolve: {ex}")
    return url

def scrape_article(url: str) -> tuple:
    try:
        # –†–∞—Å–∫—Ä—ã–≤–∞–µ–º Google-—Ä–µ–¥–∏—Ä–µ–∫—Ç
        real_url = resolve_google_url(url)

        resp = requests.get(real_url, headers=HEADERS, timeout=10)
        soup = BeautifulSoup(resp.text, "html.parser")

        # –§–æ—Ç–æ
        photo_url = ""
        og = soup.find("meta", property="og:image")
        if og and og.get("content"):
            photo_url = og["content"]
        if not photo_url:
            tw = soup.find("meta", attrs={"name": "twitter:image"})
            if tw and tw.get("content"):
                photo_url = tw["content"]

        # –£–±–∏—Ä–∞–µ–º –º—É—Å–æ—Ä
        for tag in soup(["script", "style", "nav", "footer", "header", "aside"]):
            tag.decompose()

        # –ò—â–µ–º —Ç–µ–ª–æ —Å—Ç–∞—Ç—å–∏
        article = (
            soup.find("article")
            or soup.find("div", class_=re.compile(r"article|content|body|text", re.I))
            or soup.find("main")
        )

        paragraphs = article.find_all("p") if article else soup.find_all("p")
        full_text = "\n\n".join(
            p.get_text(strip=True) for p in paragraphs if len(p.get_text(strip=True)) > 40
        )[:3000]

        log.info(f"‚úÖ {real_url[:60]} | —Ñ–æ—Ç–æ={'–µ—Å—Ç—å' if photo_url else '–Ω–µ—Ç'} | —Ç–µ–∫—Å—Ç={len(full_text)}")
        return photo_url, full_text.strip(), real_url

    except Exception as ex:
        log.error(f"‚ùå –û—à–∏–±–∫–∞: {url}: {ex}")
        return "", "", url

def get_sheet():
    scope = ["https://spreadsheets.google.com/feeds", "https://www.googleapis.com/auth/drive"]
    creds_dict = json.loads(os.environ.get("GOOGLE_CREDS"))
    creds = ServiceAccountCredentials.from_json_keyfile_dict(creds_dict, scope)
    client = gspread.authorize(creds)
    return client.open_by_key(SPREADSHEET_ID).worksheet(SHEET_NAME)

def main():
    log.info("=== –ü–∞—Ä—Å–µ—Ä —Å—Ç–∞—Ä—Ç–æ–≤–∞–ª ===")
    sheet = get_sheet()

    records = sheet.get_all_records(default_blank="")
    existing_urls = {r.get("url", "") for r in records}
    added = 0

    for feed_url in RSS_FEEDS:
        log.info(f"–ß–∏—Ç–∞—é RSS: {feed_url[:60]}")
        feed = feedparser.parse(feed_url)

        for entry in feed.entries[:10]:
            title = entry.get("title", "").strip()
            url = entry.get("link", "").strip()

            if not url or url in existing_urls:
                continue

            if not is_allowed(title):
                log.info(f"‚è≠Ô∏è –ü—Ä–æ–ø—É—Å–∫: {title[:60]}")
                continue

            photo_url, full_text, real_url = scrape_article(url)

            # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç –Ω–µ –ø–æ–ª—É—á–∏–ª–∏ ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
            if not full_text:
                log.info(f"‚è≠Ô∏è –ù–µ—Ç —Ç–µ–∫—Å—Ç–∞: {title[:60]}")
                continue

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —Ç–µ–∫—Å—Ç –Ω–µ –¥—É–±–ª–∏—Ä—É–µ—Ç –∑–∞–≥–æ–ª–æ–≤–æ–∫
            if full_text.strip().lower() == title.strip().lower():
                log.info(f"‚è≠Ô∏è –¢–µ–∫—Å—Ç = –∑–∞–≥–æ–ª–æ–≤–æ–∫: {title[:60]}")
                continue

            if not is_allowed(title + " " + full_text):
                log.info(f"‚è≠Ô∏è –§–∏–ª—å—Ç—Ä: {title[:60]}")
                continue

            sheet.append_row([
                "new",       # A - status
                title,       # B - title
                full_text,   # C - text
                photo_url,   # D - photo_url
                real_url,    # E - url (—Ä–µ–∞–ª—å–Ω—ã–π, –Ω–µ google-—Ä–µ–¥–∏—Ä–µ–∫—Ç)
                "",          # F - comment
            ])

            existing_urls.add(url)
            existing_urls.add(real_url)
            added += 1
            log.info(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ: {title[:60]}")

    log.info(f"=== –ì–æ—Ç–æ–≤–æ. –î–æ–±–∞–≤–ª–µ–Ω–æ: {added} ===")

if __name__ == "__main__":
    main()
